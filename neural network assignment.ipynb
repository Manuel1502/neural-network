{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e971ee6",
   "metadata": {},
   "source": [
    "# ------------------------------------------- First Submission -------------------------------------------\n",
    "Neural Network with Backpropagation-Learning:\n",
    "\n",
    "First reading the dataset and formatting (and imports):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2369a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random as random\n",
    "from math import exp\n",
    "from csv import reader\n",
    "\n",
    "def readFile(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            dataset.append(row)\n",
    "\n",
    "    for row in dataset[1:]:\n",
    "        for index, datapoint in enumerate(row):\n",
    "            if index < 4:\n",
    "                row[index] = int(datapoint)\n",
    "            if (index % 2) == 0 and index > 4 and index < 45:\n",
    "                row[index] = float(datapoint)\n",
    "    return dataset       \n",
    "    \n",
    "globalDataset = readFile(\"diterpene_shuf.csv\")\n",
    "globalDataset.remove(globalDataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc270be9",
   "metadata": {},
   "source": [
    "Normalizing the frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    frequencies = list()\n",
    "    for column in dataset:\n",
    "        for f in column:\n",
    "            if type(f) == float:\n",
    "                frequencies.append(f)\n",
    "    minvalue = min(frequencies)\n",
    "    maxvalue = max(frequencies)\n",
    "\n",
    "    normalizedDataset = list(dataset)\n",
    "    for row in normalizedDataset:\n",
    "        for index, datapoint in enumerate(row):\n",
    "            if (index % 2) == 0 and index > 4 and index < 45:\n",
    "                row[index] = (datapoint - minvalue)/(maxvalue-minvalue)\n",
    "    return normalizedDataset\n",
    "                \n",
    "globalDataset = normalize(globalDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96514c",
   "metadata": {},
   "source": [
    "I define a function for creating a neural network with random initial weights: (I chose only one hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617646ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetwork(numInputs, numHidden, numOutput):\n",
    "    neuralNetwork = list()\n",
    "    hiddenLayer = [{'weights':[random.random() for i in range(numInputs + 1)]} for i in range (numHidden)]\n",
    "    outputLayer = [{'weights':[random.random() for i in range(numHidden + 1)]} for i in range (numOutput)]\n",
    "    neuralNetwork.append(hiddenLayer)\n",
    "    neuralNetwork.append(outputLayer)\n",
    "    return neuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ce7c9",
   "metadata": {},
   "source": [
    "I define functions for the activation function (sigmoid function), the handling of forward and backward propagation and updating the weight values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(weights, input):\n",
    "    bias = weights[-1]\n",
    "    activationSum = bias\n",
    "    for i in range(len(input) - 1):\n",
    "        activationSum += weights[i] * input[i]\n",
    "    return 1.0 / (1.0 + exp(activationSum * -1))\n",
    "\n",
    "def forwardPropagate(neuralNetwork, data):\n",
    "    input = data\n",
    "    for layer in neuralNetwork:\n",
    "        result = list()\n",
    "        for node in layer:\n",
    "            node['output'] = activation(node['weights'], input)\n",
    "            result.append(node['output'])\n",
    "        input = result\n",
    "    return result\n",
    "\n",
    "def backwardPropagate(neuralNetwork, expectedResult):\n",
    "    for i in reversed(range(len(neuralNetwork))):\n",
    "        layer = neuralNetwork[i]\n",
    "        errors = list()\n",
    "        if i == len(neuralNetwork)-1:\n",
    "            for j in range(len(layer)):\n",
    "                errors.append(layer[j]['output'] - expectedResult[j])\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for node in neuralNetwork[i+1]:\n",
    "                    error += node['weights'][j] * node['delta']\n",
    "                errors.append(error)\n",
    "        for j in range(len(layer)):\n",
    "            node = layer[j]\n",
    "            node['delta'] = errors[j] * (node['output'] * (1.0 - node['output']))\n",
    "        \n",
    "def updateWeights(neuralNetwork, data, learningRate):\n",
    "    for i in range(len(neuralNetwork)):\n",
    "        input = data[:-1]\n",
    "        if i != 0:\n",
    "            input = [node['output'] for node in neuralNetwork[i-1]]\n",
    "        for node in neuralNetwork[i]:\n",
    "            for j in range(len(input)):\n",
    "                node['weights'][j] -= learningRate * node['delta'] * input[j]\n",
    "            #bias\n",
    "            node['weights'][-1] -= learningRate * node['delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f132452",
   "metadata": {},
   "source": [
    "My next step will be to define how to get the output of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5be0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set([data[-1] for data in globalDataset]))\n",
    "\n",
    "def cleanOutput(nodeOutput):\n",
    "    outputIndex = nodeOutput.index(max(nodeOutput))\n",
    "    return classes[outputIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496e82a",
   "metadata": {},
   "source": [
    "To avoid overfitting I use 10-fold cross validation, so I create 10 roughly equal sized splits of the (normalized) dataset:\n",
    "Also I remove any unused input from the datasetCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformatDataset(oldDataset):\n",
    "    datasetCopy = list(oldDataset)\n",
    "    newDataset = list()\n",
    "    for row in datasetCopy:\n",
    "        newEntry = list()\n",
    "        for index, data in enumerate(row):\n",
    "            if index > 4 and index < len(row) - 1:\n",
    "                if index % 2 != 0:\n",
    "                    if data == 's':\n",
    "                        newEntry.extend([row[index + 1], 0, 0, 0])\n",
    "                    elif data == 'd':\n",
    "                        newEntry.extend([0, row[index + 1], 0, 0])\n",
    "                    elif data == 't':  \n",
    "                        newEntry.extend([0, 0, row[index + 1], 0])\n",
    "                    else:\n",
    "                        newEntry.extend([0, 0, 0, row[index + 1]])\n",
    "            if index == len(row) - 1:\n",
    "                newEntry.append(data)\n",
    "        newDataset.append(newEntry)\n",
    "    return newDataset\n",
    "  \n",
    "def splitDataset(dataset, splits):\n",
    "    datasetCopy = list(dataset)\n",
    "    datasetSplit = list()\n",
    "    for i in range(splits):\n",
    "        splitSize = int(len(datasetCopy) / splits)\n",
    "        split = list()\n",
    "        while len(split) < splitSize:\n",
    "            index = random.randint(0, len(datasetCopy) - 1)\n",
    "            value = datasetCopy.pop(index)\n",
    "            split.append(value)\n",
    "        datasetSplit.append(split)\n",
    "    return datasetSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66972251",
   "metadata": {},
   "source": [
    "Lastly I define methods for training the neural network with 9 of the splits and use the tenth for validation, in 10 seperate runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(neuralNetwork, trainset, learningRate, numEpoch):\n",
    "    for epoch in range(numEpoch):\n",
    "        for data in trainset:\n",
    "            output = cleanOutput(forwardPropagate(neuralNetwork, data))\n",
    "            expected = [0 for i in range(len(classes))]\n",
    "            expected[classes.index(data[-1])] = 1\n",
    "            backwardPropagate(neuralNetwork, expected)\n",
    "            updateWeights(neuralNetwork, data, learningRate)\n",
    "\n",
    "def testNetwork(neuralNetwork, testset):\n",
    "    results = list()\n",
    "    for data in testset:\n",
    "        output = cleanOutput(forwardPropagate(neuralNetwork, data))\n",
    "        results.append(output)\n",
    "    return results\n",
    "\n",
    "def runNetwork(dataset, learningRate, numEpoch, hiddenNodes):\n",
    "    accuracy = list()\n",
    "    for split in dataset:\n",
    "        trainData = list(dataset)\n",
    "        trainData.remove(split)\n",
    "        trainData = sum(trainData, [])\n",
    "        testData = list(split)\n",
    "        network = createNetwork(len(split[0])-1, hiddenNodes, len(classes))\n",
    "        trainNetwork(network, trainData, learningRate, numEpoch)\n",
    "        results = testNetwork(network, testData)\n",
    "        correct = 0\n",
    "        for index, data in enumerate(testData):\n",
    "            if results[index] == data[-1]:\n",
    "                correct += 1\n",
    "        successRate = correct / float(len(testData)) * 100\n",
    "        print(successRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bca312",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.5\n",
    "numEpoch = 1200\n",
    "hiddenNodes = 12\n",
    "splitAmount = 10\n",
    "\n",
    "datasetSplit = splitDataset(reformatDataset(globalDataset), splitAmount)\n",
    "\n",
    "runNetwork(datasetSplit, learningRate, numEpoch, hiddenNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac0ee0",
   "metadata": {},
   "source": [
    "The average accuracy with this configuration is between 45% and 50% which isn't great but it is somewhat working and I will try to further improve it for the second part of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabcb24",
   "metadata": {},
   "source": [
    "# ------------------------------------------- Improvements-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e53666",
   "metadata": {},
   "source": [
    "To improve accuracy and be indepent of the order I want to try a few Ideas:\\\n",
    "First I combine the frequency and its multiplicity into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must be done before reformatDataset\n",
    "#also removes tha first 4 datapoints (expert data)\n",
    "def combineDatapoints(dataset):\n",
    "    newDataset = list()\n",
    "    for row in dataset:\n",
    "        newEntry = list()\n",
    "        for index, data in enumerate(row):\n",
    "            if index > 4 and index < len(row) - 1:\n",
    "                if index % 2 != 0:\n",
    "                    newEntry.append([row[index], row[index + 1]])\n",
    "            if index == len(row) - 1:\n",
    "                newEntry.append(data)\n",
    "        newDataset.append(newEntry)\n",
    "    return newDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e64cb",
   "metadata": {},
   "source": [
    "Accuracy will be improved by sorting dataset by frequencies (ascending):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10351de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must be done before reformatDataset\n",
    "def sortDataset(dataset):\n",
    "    dataset = combineDatapoints(dataset)\n",
    "    sortedDataset = list()\n",
    "    for row in dataset:\n",
    "        newEntry = list(row)\n",
    "        classification = newEntry.pop()\n",
    "        newEntry = sorted(newEntry, key = lambda x : (x[1], x[0]))\n",
    "        newEntry.append(classification)\n",
    "        sortedDataset.append(newEntry)\n",
    "    return sortedDataset\n",
    "    \n",
    "def reformatCombined(dataset):\n",
    "    datasetCopy = list(dataset)\n",
    "    newDataset = list()\n",
    "    for row in datasetCopy:\n",
    "        newEntry = list()\n",
    "        for index, data in enumerate(row):\n",
    "            if index < len(row) - 1:\n",
    "                if data[0] == 's':\n",
    "                    newEntry.extend([data[1], 0, 0, 0])\n",
    "                elif data[0] == 'd':\n",
    "                    newEntry.extend([0, data[1], 0, 0])\n",
    "                elif data[0] == 't':  \n",
    "                    newEntry.extend([0, 0, data[1], 0])\n",
    "                else:\n",
    "                    newEntry.extend([0, 0, 0, data[1]])\n",
    "            if index == len(row) - 1:\n",
    "                newEntry.append(data)\n",
    "        newDataset.append(newEntry)\n",
    "    return newDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.5\n",
    "numEpoch = 1200\n",
    "hiddenNodes = 12\n",
    "splitAmount = 10\n",
    "\n",
    "sortedDataset = sortDataset(globalDataset)\n",
    "sortedSplit = splitDataset(reformatCombined(sortedDataset), splitAmount)\n",
    "\n",
    "runNetwork(sortedSplit, learningRate, numEpoch, hiddenNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992397c3",
   "metadata": {},
   "source": [
    "Another idea I wanted to try out was summing up the multiplicities and using the average of frequencies, this didn't really work out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474bd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must be done before reformatDataset\n",
    "def sumDataset(dataset):\n",
    "    dataset = combineDatapoints(dataset)\n",
    "    summedDataset = list()\n",
    "    for row in dataset:\n",
    "        newEntry = list()\n",
    "        amountS = 0\n",
    "        amountD = 0\n",
    "        amountT = 0\n",
    "        amountQ = 0\n",
    "        sumS = 0.0\n",
    "        sumD = 0.0\n",
    "        sumT = 0.0\n",
    "        sumQ = 0.0\n",
    "        for index, data in enumerate(row):\n",
    "            if index < len(row) - 1:\n",
    "                if data[0] == 's':\n",
    "                    amountS += 1\n",
    "                    sumS += data[1]\n",
    "                elif data[0] == 'd':\n",
    "                    amountD += 1\n",
    "                    sumD += data[1]\n",
    "                elif data[0] == 't':  \n",
    "                    amountT += 1\n",
    "                    sumT += data[1]\n",
    "                else:\n",
    "                    amountQ += 1\n",
    "                    sumQ += data[1]\n",
    "            if index == len(row) - 1:\n",
    "                avgS = sumS/amountS\n",
    "                avgD = sumD/amountD\n",
    "                avgT = sumT/amountT\n",
    "                avgQ = sumQ/amountQ\n",
    "                newEntry.extend([amountS, avgS, amountD, avgD, amountT, avgT, amountQ, avgQ])\n",
    "                newEntry.append(data)\n",
    "        summedDataset.append(newEntry)\n",
    "    return summedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0babc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.5\n",
    "numEpoch = 1600\n",
    "hiddenNodes = 8\n",
    "splitAmount = 10\n",
    "\n",
    "summedSplit = splitDataset(sumDataset(globalDataset), splitAmount)\n",
    "\n",
    "runNetwork(summedSplit, learningRate, numEpoch, hiddenNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b689ec10",
   "metadata": {},
   "source": [
    "I also wanted to look if sorting by multiplicities first makes a difference: \\\n",
    "It seems to be about 5% less accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must be done before reformatDataset\n",
    "def sortByMultiplicities(dataset):\n",
    "    dataset = combineDatapoints(dataset)\n",
    "    mSortedDataset = list()\n",
    "    for row in dataset:\n",
    "        newEntry = list(row)\n",
    "        classification = newEntry.pop()\n",
    "        newEntry = sorted(newEntry, key = lambda x : (x[0], x[1]))\n",
    "        newEntry.append(classification)\n",
    "        mSortedDataset.append(newEntry)  \n",
    "    return mSortedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.5\n",
    "numEpoch = 1200\n",
    "hiddenNodes = 12\n",
    "splitAmount = 10\n",
    "\n",
    "mSortedSplit = splitDataset(reformatCombined(sortByMultiplicities(globalDataset)), splitAmount)\n",
    "\n",
    "runNetwork(mSortedSplit, learningRate, numEpoch, hiddenNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f33e8",
   "metadata": {},
   "source": [
    "Last I wanted to see how effective randomizing the order after every training epoch is, but as expected it didn't show much improvement compared to doing nothing with the data at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13845d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizedCopy(data):\n",
    "    newData = list()\n",
    "    for row in data:\n",
    "        newData.append(randomize(row))\n",
    "    return newData\n",
    "\n",
    "def randomize(dataRow):\n",
    "    newEntry = list()\n",
    "    for index, data in enumerate(dataRow):\n",
    "        if index < len(dataRow) - 1:\n",
    "            if index % 2 == 0:\n",
    "                newEntry.append([dataRow[index], dataRow[index + 1]])\n",
    "    random.shuffle(newEntry)\n",
    "    flatEntry = [item for sublist in newEntry for item in sublist]\n",
    "    flatEntry.append(dataRow[-1])\n",
    "    return flatEntry\n",
    "\n",
    "def trainRandomizedNetwork(neuralNetwork, trainset, learningRate, numEpoch):\n",
    "    for epoch in range(numEpoch):\n",
    "        for data in trainset:\n",
    "            output = cleanOutput(forwardPropagate(neuralNetwork, data))\n",
    "            expected = [0 for i in range(len(classes))]\n",
    "            expected[classes.index(data[-1])] = 1\n",
    "            backwardPropagate(neuralNetwork, expected)\n",
    "            updateWeights(neuralNetwork, data, learningRate)\n",
    "        trainset = randomizedCopy(trainset)\n",
    "\n",
    "learningRate = 0.5\n",
    "numEpoch = 1200\n",
    "hiddenNodes = 12\n",
    "splitAmount = 10\n",
    "\n",
    "datasetSplit = splitDataset(reformatDataset(globalDataset), splitAmount)\n",
    "\n",
    "accuracy = list()\n",
    "for split in datasetSplit:\n",
    "    trainData = list(datasetSplit)\n",
    "    trainData.remove(split)\n",
    "    trainData = sum(trainData, [])\n",
    "    testData = list(split)\n",
    "    network = createNetwork(len(split[0])-1, hiddenNodes, len(classes))\n",
    "    trainRandomizedNetwork(network, trainData, learningRate, numEpoch)\n",
    "    results = testNetwork(network, testData)\n",
    "    correct = 0\n",
    "    for index, data in enumerate(testData):\n",
    "        if results[index] == data[-1]:\n",
    "            correct += 1\n",
    "    successRate = correct / float(len(testData)) * 100\n",
    "    print(successRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7167a2d",
   "metadata": {},
   "source": [
    "So the best method I tried out was simply sorting the dataset in order of frequencies, which resulted in around 80% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
